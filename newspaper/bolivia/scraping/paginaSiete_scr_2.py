from scrapy.item import Field
from scrapy.item import Item
from scrapy.spiders import CrawlSpider, Rule
from scrapy.selector import Selector
from scrapy.loader.processors import MapCompose, Join
from scrapy.linkextractors import LinkExtractor
from scrapy.loader import ItemLoader
from scrapy.crawler import CrawlerProcess
 

class Periodico(Item):
    titulo = Field()
    fecha = Field()
    hora = Field()

class paginaSiete(CrawlSpider):
    name = "PáginaSiete"
    allowed_domains = ['www.paginasiete.bo'] # Me aseguro que no irán a páginas que no sean de este dominio
    custom_settings = {
        'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/71.0.3578.80 Chrome/71.0.3578.80 Safari/537.36',
        'FEED_EXPORT_FIELDS': ["titulo","fecha","hora"],
        #'CLOSESPIDER_PAGECOUNT': 20,
        #'CLOSESPIDER_ITEMCOUNT': 20,
        'CONCURRENT_REQUESTS': 1 # numero de requerimientos concurrentes
    }

    download_delay = 1

    start_urls = [
            'https://www.paginasiete.bo/archivo/-/search/%5E/false/false/19700101/20221104/date/false/false/c2VjdGlvbk5hbWU6MMKnODBjMWViODktYTdmZC00M2EwLThhOTQtZmYwOTQ5Njg2MmQwKg%3D%3D/0/meta/0/13431522-106982-8969384-107102-106974-106980-106990-106992-265456-234764-13433769-106986-106978-106976-106988-235476-229328-106984-3665695-11675327-229325/0/' + str(i) for i in range(1,4316)
            ]

    rules = (
        #paginación
        Rule(
            LinkExtractor(
                allow = r'/archivo/-/search/%5E/false/false/19700101/20221104/date/false/false/c2VjdGlvbk5hbWU6MMKnODBjMWViODktYTdmZC00M2EwLThhOTQtZmYwOTQ5Njg2MmQwKg%3D%3D/0/meta/0/13431522-106982-8969384-107102-106974-106980-106990-106992-265456-234764-13433769-106986-106978-106976-106988-235476-229328-106984-3665695-11675327-229325/0/\d+'
            ), follow = True, callback = 'parse_items'
        ),
    )

    def quitarEspacio(self, fecha):
        fecha = fecha.replace('\n', '')
        return fecha.strip()

    def espacio(self, hora):
        return hora.strip()

    def lista(self, lists):
        if len(lists) > 1:
            return "".join(lists)
        else:
            return lists[0]


    def parse_items(self, response):
        sel = Selector(response)
        opiniones = sel.xpath('//*[@id="3128922993"]/ul/li/div')

        for opinion in opiniones:
            item = ItemLoader(Periodico(), opinion)
            ## condición if para dos paths diferentes

            item.add_xpath('titulo', 'div[3]/div[2]/div/div[1]/a/h2/text()',
                            MapCompose(self.espacio,self.lista)
            )

            item.add_xpath('titulo', 'div[3]/div[3]/a/h2/text()',
                            MapCompose(self.espacio,self.lista)
            )
            item.add_xpath('fecha', 'div[1]/text()',
                            MapCompose(self.quitarEspacio)
            )
            item.add_xpath('hora', 'div[3]/div[1]/h2/text()',
                            MapCompose(self.espacio)
            )

            yield item.load_item()


process = CrawlerProcess({
    'FEED_FORMAT': 'json',         # formato del archivo generado
    'FEED_URI': '../data/paginaSiete_prueba.json',  # nombre del archivo generado
    'FEED_EXPORT_ENCODING': 'utf-8'
    })
process.crawl(paginaSiete)
process.start() # the script will block here until the crawling is finished
